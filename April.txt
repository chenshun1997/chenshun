2021.04.21
看《向上生长》
      一个人在读每本书的过程中，都会有个消化阶段，也就是说怎么让这些知识在自己的脑海里溜达一圈，吸收一下。我的方法就是把收获的知识发一条微博，再加上一点之前学到的东西，连接到自己的”认知树“。   一个人想要输出，必须依赖积累，这本来就是常识。

Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths 基于梯度优化和拓扑路径的鲁棒实时无人机重规划
   ①基于梯度的轨迹优化
   ②拓扑路径规划
   ③路径引导轨迹优化
   ④拓扑路径搜索

2021.04.22
  基于梯度优化和拓扑路径的鲁棒实时无人机重规划：首先生成全局参考路径(多项式轨迹规划(实验中采用的)、RRT、TRR和Ewok(文章中提到的))，在飞行过程中9m地平线内的局部轨迹被重新规划，以避开以前未知的障碍物，同时保证无人机靠近全局轨迹。
  PGO(geometric path in the optimization)在优化中加入几何路径，①考虑轨迹的平滑成本，有B样条的控制点在引导；②惩罚引导路径和B样条之间的距离；③考虑ESDF的碰撞成本以及不可行的速度和加速度的惩罚
  总体来说：文章前面介绍的方法很高大上，后面的实验用的是多项式轨迹规划，（由于在实验中需要考虑不可行的速度和加速度，全局的直线引导和局部的优化，其实并不存在陷入局部极小值的麻烦吧）

  ESDF(Euclidean Signed Distance Fields)欧氏符号距离场


  哈工大的操作系统课是本校CS课程中含金量最高的，尤其是实验课。

  Teach-Repeat-Replan: A Complete and Robust System for Aggressive Flight in Complex Environments   示教-重复-重放:复杂环境中攻击性飞行的完整而鲁棒的系统
<内容未能完全保存>

2021.04.23

注意力和肌肉一样，存在损耗。强制自己在某一件事上集中一段时间，就发现自己在别的事情上注意力涣散；；            ”反自然“让你往不利于自己的方向发展是自然趋势，你必须能够克制自己的惰性与随心所欲，才能让自己不断地朝好的方向发展。；；         像马桶里的水流有个小漩涡，水分子本身并没有漩涡这个特点，只有大量的水分子挤在一起才有。 ‘ 高速公路化’   ；；； 努力 是希望你能主动出击，克服恐惧，迎接挑战；；我们讲逻辑讲人心，唯独不讲道德善恶。  承认平庸，肯下笨功夫

Path Deformation Roadmaps: Compact Graphs with Useful Cycles for Motion Planning路径变形路线图:具有运动规划有用循环的紧凑图

  第一阶段是基于可见度-PRM，以构建一个小树覆盖空间，并尽可能捕捉其连接的组件。
  第二个阶段旨在用创建有用周期所涉及的新节点来丰富路线图。这一步的关键要素是有效的路径可见性测试，用于过滤无用的路径，这些路径很容易变形为现有的路线图路径。遵循PRM可见度的哲学，第二阶段还整合了一个基于难以找到新的有用周期的停止条件。

probabilistic roadmap planner (PRM)的总体原则是捕捉无碰撞空间的连通性 存储在预先计算的路线图中的三维曲线；路线图是通过对机器人配置进行采样，然后将有希望的样本与简单快速的本地规划器生成的有效本地路径连接起来而获得的

a Roadmap Connected from any Point of View(RCPV roadmaps) 从任何角度连接的路线图（称为RCPV路线图）

路径变形路线图 Path Deformation Roadmaps (PDRs)
Probabilistic Roadmap Planner (PRM) 概率路线图规划



2021.04.25

长期暗中观察，整体来说想发财有两个必要的条件：一是可以全年无休；二是坚定态度。我发现混得好的人，一方面知道生意难做，另一方面又坚定态度。
大家往往高估自己一天能学会的东西，低估三年能学会的东西。

函数的声明说的是这个函数指定存在，不管在那个文件中；定义是说这个函数是干什么的，这个是函数的主题；



Lifelong Planning A*算法

Multi-agent navigation based on deep reinforcement learning and traditional pathfinding algorithm
传统方法和强化学习结合的多智能体路径规划
A*+RL 总体上来说就是用强化学习来执行传统的pathfinding algorithm的动作

2021.04.26

习得性无助

多智能体的框架能够在抽象的新场景中达到终点，，用Unity3D和Tensorflow 来为场景构建模型和环境；可以在不同的情况下附加在不同的环境中，特别是当规模很大的时候。

学习离散动作，加入之前的动作（本篇文章改进的地方）
state ：
作者在unity3D上的简要环境(在一个游戏引擎上做的实验吧)，一个机器人带有45方向个传感器，感知范围为d；；如果某个方向上存在障碍，感知到的东西为（0，di）；如果是其他智能体，感知到的东西就是（1，di）状态相当简单。
reward
奖励机制：
 rt = r navigation+r scenario+r penalty
①传统的A*（pathfinding algorithm）如果是a0就会引入一个正的奖励
②如果采用的动作撞到了障碍物或者墙壁或者其他智能体，就会扣一个比较大的分数，到了终点+一个正分；
③惩罚step time 设置r0=0.00005 (其实并不觉得会见好)
PPO使用的机器学习的算法

待解决问题：
1.感觉应该给state加A*轨迹信息；
2.每时每刻都需要给所有的agents都重新规划一条路径，计算量会不会特别大
agents采用除了a0以外的动作，可能走到新的位置上，原来的位置规划的A*就不能用了，，，，，，，

紧接着看这片文章
Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments

state：‘
三个channels
1.current observed static obstacles，障碍物和其他车辆的位置；
2.其他车的轨迹序列，同一辆车的轨迹序列的值，不同时刻不同；
3.A*规划的路径，并不是每时每刻都进行规划，那样的话太费时间了，
参考路径的更新频率可能比我们基于强化学习的本地计划程序低的多。

2021.04.27

对啊，反正什么事都会发生
所谓的内卷化，指的是一种社会（文化模式）某一发展阶段达到某种确定的形式之后，这种形式停滞不前，难以转化为另一种高级模式的现象，从而把自我锁死在低水平状态上，周而复始地循环。




首先，深度强化学习只是保证了无碰撞的路径，而不是最短路经，这些算法并不考虑映射的全局结构（agents的活动区域），这会使得最终的路径没有得到全区的优化。其次，对agent的训练与测试只能在有限类型的场景下进行，这会导致对环境方差的弱鲁棒性；；如果环境发生变化的时候，有可能获得更高的碰撞率或更长的平均路径。并且每次的训练数据并不会少，导致训练的时间代价很大。

num1：将两种方案结合，用传统的方法进行寻路，通过机器学习的方法进行避障。（结合多智能体强化学习和经典的寻路算法）

我们使用强化学习来训练代理的避免碰撞行为策略，同时附加一个路径查找算法来引导代理到他们的目标。代理学会在每一步决定是导航还是避开他们的合作伙伴。场景中的每个代理都独立操作，而培训是集中——所有代理都共享相同的策略πθ和参数集θ。

主要的想法是使决策策略高度适应未知或随机的环境 
 引入随机决策过程，以减少代理之间的冲突，提高整个系统的协同作用



布尔值的类型：0为假，其他数字都为真（整数）。  常数迭代(constant folding)，在预编译的时候进行赋值操作，将所有的值转移到一个变量中，编译器优化 。



2021.04.28



这种寻路算法需要将地图进行预处理，即将由地图边界拐点和障碍物外围拐点所形成的离散点的集合转换成为一个全部由三角型组成的网络，这种处理叫做“三角剖分”，其中最著名的应该就是Delaunay三角剖分了.
接下来假设我们的人物在点A处，我们用鼠标点击了B点，这时我们希望将人物从点A移动到点B，于是我们首先用A*算法查找一条最近的网格路径，在A*寻路过程中人物在三角形中的位置我们默认是三角形的重心，即三角形三边中线的交点，因为这一点必然在三角形内部，且基本为三角形比较中间的位置。于是我们找到了如图1-3所示的一条三角形路径：
其中黑色线条表示A*寻径算法计算出来的初始路径，首先从A点出发走到A点所在三角形的重心，然后依次移动到下一个三角形的重心，最后移动到B点。不难发现这种移动远没有达到我们预期的结果，所以最后还需要一步平滑的过程。
。黑色线条中间夹着的范围类似于动物和人的“视野”；蓝色线条表示一次“远望”，可以看到针对左边视野边界的第一次远望的目标点在原视野范围内，因此不做移动，只将左边视野范围调整到这次远望的目标点处；接下来的两次远望同第一次一样在视野范围内，因此只调整视野范围而不做移动；接着再针对右边视野进行一次远望，这次发现目标点不在原视野范围内了，因此需要将人物从A点移动到C点才可以看到该点。黄线表示人物移动的路线。然后再从C点确定新的视野范围，以此类推，可到达B点的路线用黄颜色的折线连接而成，最后通过回溯找到这条路径完整的过程，这样就完成了我们的寻路，可以看到使用基于网格的寻路算法找到的路径基本上就是最近的路线了。

实际上三角剖分为我们A*寻路和最后的平滑处理过程提供了很大的帮助，尤其是平滑过程，使用三角形来处理“视野”和“远望”是一个很好的做法，因为三角形只有三个角，因而可以很容易找到下一次远望的目标点——只需要将某一边的视野沿着外侧的边移动到下一个三角形唯一没有被使用过的点上即可，至于到底移动哪一边视野，需要保证移动后两边视野的落脚点之间有边连接。

这种寻路算法相对于A*有很明显的优势：

1. 能够找出真正最近的路径；

2. Delaunay三角剖分的结果比按照矩形或者六边形等固定形状进行拆分的结果更能够描述复杂的地形；

3. Delaunay三角剖分出来的网格数量比按照固定形状拆分出来的格子数量少，减少了寻路开销。

2021.04.29

1.多边形生成三角形http://sites-final.uclouvain.be/mema/Poly2Tri/poly2tri.html


2.凸多边形网格构建：recastnavigation https://github.com/memononen/recastnavigation

3.凸多边形网络三角形划分：http://www.critterai.org/nmgen_polygen

4.路径优化拐点法： http://www.navpower.com/gdc2006_miles_david_pathplanning.ppt

A*求出路径后，并不选取这条路径，而是选择该路径下的所有网格，然后在网格内用拐点法，找拐点，连接所有拐点得到最后路径。那要是这样的话，得到的一定会是最优路径吗？

一、整体思路NavMesh
1、整个寻路的网格是由多个互相连接的凸多边形组成

2、初始化网格的时候，计算出凸多边形互相相邻的边，和通过互相相邻的边到达另外一个多边形的距离。

3、开始寻路时，首先肯定是会得到一个开始点的坐标和结束点的坐标

4、判断开始点和结束点分别位于哪个多边形的内部（只通过x和z坐标，高度先忽略掉），把多边形的编号记录下来

5、使用A*寻路，找到从开始的多边形到结束的多边形将会经过哪几个多边形，记录下来。

6、在得到途径的多边形后， 从开始点开始，根据拐点算法，计算出路径的各个拐点组成了路径点坐标。（忽略高度，只计算出x和z）。

7、到上一步，2D寻路部分结束。人物根据路径点做移动。

8、假如需要3D高度计算，那么在获得了刚才2D寻路的路径点之后，再分别和途径的多边形的边做交点计算，得出经过每一个边时的交点，那么当多边形与多边形之间有高低变化，路径点也就通过边的交点同样的产生高度的变化。
二、详细算法
1、A*寻路得出途径的多边形：

根据A*寻路的估价

F = G + H

F是格子的总消耗

G是走到下一个格子的消耗的值

H是格子到终点的理论消耗值

在这里，G等于是当前多边形与下一个多边形中心点的距离，这个距离在初始化的时候会先算出来，以后只需要根据交接的边就能取出这个距离。

H其实就是下一个多边形到终点的直线距离了。

之后就是普通的A*算法了，为每一个格子建立开启列表和关闭列表，然后判断开启列表里面每一个多边形的F值，选取最小的作为下一个格子，如此类推得到了所走过的所有多边形id。

我暂时写好的算法并不是寻找最小路径的，而是用G和H值最快的得出一条可走的路径。这是因为做最小路径必须要历遍所有的开启列表，会比较耗时间。但假如只通过G和H来找路径，有可能会找出绕远路的路径。这时候就要衡量一下G和H的权重了。我这里是把H的权重加大，让方向性比走到下一个多边形的消耗更优先的作为考虑的依据。

nav，即navigation，现行3D游戏主流寻路方式，起源的思路是和A*完全不同的，因为navmesh不需要一张二维表，只需要利用模型阻挡生成一张近似寻路用的“mesh”





